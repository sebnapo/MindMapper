{
  "name":"machine learning",
  "children":[
    {
      "name":" university oxford",
      "note":"coucou",
      "address":"wikipedia.org",
      "children":[
        {
          "name":"Introduction to Algorithms",
          "note":"This course provides an introduction to mathematical modeling of computational problems. It covers the common algorithms, algorithmic paradigms, and data structures used to solve these problems. The course emphasizes the relationship between algorithms and programming, and introduces basic performance measures and analysis techniques for these problems.",
          "address":"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/MIT6_006F11_lec14_orig.pdf"
        },
        {
          "name":"Probabilistic Systems Analysis and Applied Probability",
          "note":"Welcome to 6.041/6.431, a subject on the modeling and analysis of random phenomena and processes, including the basics of statistical inference. Nowadays, there is broad consensus that the ability to think probabilistically is a fundamental component of scientific literacy. For example:      The concept of statistical significance (to be touched upon at the end of this course) is considered by the Financial Times as one of &quot;The Ten Things Everyone Should Know About Science&quot;.     A recent Scientific American article argues that statistical literacy is crucial in making health-related decisions.     Finally, an article in the New York Times identifies statistical data analysis as an upcoming profession, valuable everywhere, from Google and Netflix to the Office of Management and Budget.  The aim of this class is to introduce the relevant models, skills, and tools, by combining mathematics with conceptual understanding and intuition.",
          "address":"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/recitations/MIT6_041F10_rec23_sol.pdf"
        },
        {
          "name":"Probabilistic Systems Analysis and Applied Probability",
          "note":"This course introduces students to the modeling, quantification, and analysis of uncertainty.&nbsp; The tools of probability theory, and of the related field of statistical inference, are the keys for being able to analyze and make sense of data. These tools underlie important advances in many fields, from the basic sciences to engineering and management.",
          "address":"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/unit-iv/MIT6_041SCF13_rec23_sol.pdf"
        },
        {
          "name":"Cellular Neurobiology",
          "note":"This course serves as an introduction to the structure and function of the nervous system. Emphasis is placed on the cellular properties of neurons and other excitable cells. Topics covered include the structure and biophysical properties of excitable cells, synaptic transmission, neurochemistry, neurodevelopment, and the integration of information in simple systems and the visual system.",
          "address":"https://ocw.mit.edu/courses/biology/7-29j-cellular-neurobiology-spring-2012/lecture-notes/MIT7_29JS12_lecture22.pdf"
        }
      ]
    },
    {
      "name":" learning library",
      "note":"coucou",
      "address":"wikipedia.org",
      "children":[
        {
          "name":"Learning Seminar: Experiments in Education",
          "note":"This seminar explores experiments in education and discusses how education and learning might be done, through reading and discussion. This seminar is not&nbsp;a survey of experiments in education, but rather, its goal is to determine how learning should happen and what kinds of contexts allow it to happen.",
          "address":"https://ocw.mit.edu/courses/experimental-study-group/es-291-learning-seminar-experiments-in-education-spring-2003/assignments/MITES_291S03_5.pdf"
        },
        {
          "name":"Designing and Leading the Entrepreneurial Organization",
          "note":"This subject is about building, running, and growing an organization. Subject has four central themes:  How to think analytically about designing organizational systems How leaders, especially founders, play a critical role in shaping an organization's culture What really needs to be done to build a successful organization for the long-term and What one can do to improve the likelihood of personal success.  Not a survey of entrepreneurship or leadership; subject addresses the principles of organizational architecture, group behavior and performance, interpersonal influence, leadership and motivation in entrepreneurial settings. Through a series of cases, lectures, readings and exercises students develop competencies in organizational design, human resources management, leadership and organizational behavior in the context of a new, small firm.",
          "address":"https://ocw.mit.edu/courses/sloan-school-of-management/15-394-designing-and-leading-the-entrepreneurial-organization-spring-2003/lecture-notes/buildingalearningorganization.pdf"
        },
        {
          "name":"Statistical Generalization of Robot Movement in Autoencoder Latent Space",
          "note":"Robots have started to emerge in different environments, like for example in households and medical care.\r\nIn such environments they cannot be preprogramed for all possible tasks. One solution is to equip the\r\nrobots with the ability to learn, so they can adapt their behaviour to ever changing environments. One of\r\nthe learning approaches is imitation learning, where a robot is given human demonstrations of the desired\r\ntask execution. Typically using several demonstrations trajectories, the robot can extract suitable parameters\r\nto generate movement commands for its own motion. We used imitation learning to teach a robot to\r\naccurately throw a ball. In this case the robot is given a set of example throws and information where the\r\nball has landed. The landing positions are used as task parameters. For every new target position, the robot\r\nshould compute a new arm movement that is similar to the example movements and results in the ball\r\nhitting the target. Robot movement trajectories are described in joint coordinates. In our work we applied\r\ndeep autoencoders to learn a representation of robot movements in latent coordinates. Latent spaces are\r\nuseful because they provide a representation of robot movement in a lower dimensional space. An\r\nautoencoder is a neural networks made of an encoder and decoder part. The encoder part takes as input a\r\nconfiguration on the robot trajectory in joint space. The data are encoded through layers of the encoder\r\nnetwork, where each layer has less neurons than the previous one. From the latent space the decoder\r\nnetwork can transform latent space coordinates back to the original joint space. An autoencoder is usually\r\ntrained using backpropagation.\r\n\r\nIn our research, we applied a deep autoencoder to lower the dimensionality of robot joint space. Statistical\r\nlearning is applied to latent space representations to compute an optimal trajectory to throw a ball at a given\r\ntarget. In the first step, the example trajectories are transformed to their latent space representations and\r\nencoded with dynamic motion primitives (DMP). For every new throw, new DMP parameters in latent\r\nspace are then computed with Gaussian Process Regression (GPR) or Locally Weighted Regression (LWR).\r\nFrom the computed DMP parameters, the trajectory that can be executed by a robot can be computed using\r\nthe decoder part of the autoencoder.\r\n\r\nThe proposed method was evaluated in simulation of a three degrees of freedom planar robot, throwing a\r\nball at target located at different horizontal and vertical positions. Generalization of robotic throws was\r\nperformed using LWR and GPR. We compared the performance of statistical learning in the original robot\r\njoint space and in latent space. Statistical learning in the original joint space was in comparison with learning\r\nin latent space computationally slower, but with higher accuracy. The best accuracy was achieved with GPR\r\nin joint space.",
          "address":"http://hydro.ijs.si/v014/2c/frsogpzin6cpbhnj3xqyjfjmufpac6sd.pdf"
        }
      ]
    },
    {
      "name":" learning optimization",
      "note":"coucou",
      "address":"wikipedia.org",
      "children":[
        {
          "name":"Statistical Generalization of Robot Movement in Autoencoder Latent Space",
          "note":"Robots have started to emerge in different environments, like for example in households and medical care.\r\nIn such environments they cannot be preprogramed for all possible tasks. One solution is to equip the\r\nrobots with the ability to learn, so they can adapt their behaviour to ever changing environments. One of\r\nthe learning approaches is imitation learning, where a robot is given human demonstrations of the desired\r\ntask execution. Typically using several demonstrations trajectories, the robot can extract suitable parameters\r\nto generate movement commands for its own motion. We used imitation learning to teach a robot to\r\naccurately throw a ball. In this case the robot is given a set of example throws and information where the\r\nball has landed. The landing positions are used as task parameters. For every new target position, the robot\r\nshould compute a new arm movement that is similar to the example movements and results in the ball\r\nhitting the target. Robot movement trajectories are described in joint coordinates. In our work we applied\r\ndeep autoencoders to learn a representation of robot movements in latent coordinates. Latent spaces are\r\nuseful because they provide a representation of robot movement in a lower dimensional space. An\r\nautoencoder is a neural networks made of an encoder and decoder part. The encoder part takes as input a\r\nconfiguration on the robot trajectory in joint space. The data are encoded through layers of the encoder\r\nnetwork, where each layer has less neurons than the previous one. From the latent space the decoder\r\nnetwork can transform latent space coordinates back to the original joint space. An autoencoder is usually\r\ntrained using backpropagation.\r\n\r\nIn our research, we applied a deep autoencoder to lower the dimensionality of robot joint space. Statistical\r\nlearning is applied to latent space representations to compute an optimal trajectory to throw a ball at a given\r\ntarget. In the first step, the example trajectories are transformed to their latent space representations and\r\nencoded with dynamic motion primitives (DMP). For every new throw, new DMP parameters in latent\r\nspace are then computed with Gaussian Process Regression (GPR) or Locally Weighted Regression (LWR).\r\nFrom the computed DMP parameters, the trajectory that can be executed by a robot can be computed using\r\nthe decoder part of the autoencoder.\r\n\r\nThe proposed method was evaluated in simulation of a three degrees of freedom planar robot, throwing a\r\nball at target located at different horizontal and vertical positions. Generalization of robotic throws was\r\nperformed using LWR and GPR. We compared the performance of statistical learning in the original robot\r\njoint space and in latent space. Statistical learning in the original joint space was in comparison with learning\r\nin latent space computationally slower, but with higher accuracy. The best accuracy was achieved with GPR\r\nin joint space.",
          "address":"http://hydro.ijs.si/v014/2c/frsogpzin6cpbhnj3xqyjfjmufpac6sd.pdf"
        },
        {
          "name":"Regression Canonical Correlation Analysis",
          "note":"In this paper we present Regression Canonical Correlation Analysis, an extension of Canonical Correlation Analysis, where one of the dimensions is fixed and demonstrate how it can be solved efficiently. We applied the extension to the task of query translation in the context of Cross-Lingual Information Retrieval.",
          "address":"http://hydro.ijs.si/v004/3d/hxngtov4yujsnpumlntgvcxif7kqju6r.pdf"
        }
      ]
    },
    {
      "name":" supervised learn",
      "note":"coucou",
      "address":"wikipedia.org",
      "children":[
        {
          "name":"Introduction to Geology",
          "note":"Geology is the core discipline of the earth sciences and encompasses many different phenomena, including plate tectonics and mountain building, volcanoes and earthquakes, and the long-term evolution of Earth&rsquo;s atmosphere, surface and life. Because of the ever-increasing demand for resources, the growing exposure to natural hazards, and the changing climate, geology is of considerable societal relevance. This course introduces students to the basics of geology. Through a combination of lectures, labs, and field observations, we will address topics ranging from mineral and rock identification to the origin of the continents, from geologic mapping to plate tectonics, and from erosion by rivers and glaciers to the history of life.",
          "address":"https://ocw.mit.edu/courses/earth-atmospheric-and-planetary-sciences/12-001-introduction-to-geology-fall-2013/lecture-notes-and-slides/MIT12_001F13_Lecture11slides.pdf"
        },
        {
          "name":"Atmospheric and Ocean Circulations",
          "note":"In this course, we will look at many important aspects of the circulation of the atmosphere and ocean, from length scales of meters to thousands of km and time scales ranging from seconds to years. We will assume familiarity with concepts covered in course&#160;12.003 (Physics of the Fluid Earth). In the early stages of the present course, we will make somewhat greater use of math than did 12.003, but the math we will use is no more than that encountered in elementary electromagnetic field theory, for example. The focus of the course is on the physics of the phenomena which we will discuss.",
          "address":"https://ocw.mit.edu/courses/earth-atmospheric-and-planetary-sciences/12-333-atmospheric-and-ocean-circulations-spring-2004/lecture-notes/ch7.pdf"
        },
        {
          "name":"Physics I: Classical Mechanics",
          "note":"This class is an introduction to classical mechanics for students who are comfortable with calculus. The main topics are: Vectors, Kinematics, Forces, Motion, Momentum, Energy, Angular Motion, Angular Momentum, Gravity, Planetary Motion, Moving Frames, and the Motion of Rigid Bodies.",
          "address":"https://ocw.mit.edu/courses/physics/8-012-physics-i-classical-mechanics-fall-2008/related-resources/final_f05sol.pdf"
        },
        {
          "name":"Calculus Online Textbook",
          "note":"Published in 1991 by Wellesley-Cambridge Press, the book is a useful resource for educators and self-learners alike. It is well organized, covers single variable and multivariable calculus in depth, and is rich with applications.&nbsp; In addition to the Textbook, there is also an online Instructor's Manual and a student Study Guide. Prof. Strang has also developed a related series of videos, Highlights of Calculus, on the basic ideas of calculus.The 2010 second edition of the Calculus textbook includes a new chapter on &quot;Highlights of Calculus&quot; that connects to the video series of the same name.&nbsp; The new chapter has summaries and practice questions for all of the videos.&nbsp; It also introduces The Exponential Function (e^x) as presented in Prof. Strang's video on this topic.",
          "address":"https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/MITRES_18_001_strang_9.pdf"
        }
      ]
    }
  ]
}